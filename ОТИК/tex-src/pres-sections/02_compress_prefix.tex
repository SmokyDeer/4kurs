

\section{Источник без памяти}
\subsection{Модель источника --- стационарный без памяти}
\begin{frame}{\insertsubsection}
% \small
% \footnotesize
\setlength{\parskip}{0.\parskip}

Модель источника $X$ "--- стационарный источник без памяти,\\ строится по~кодируемому сообщению~$C$:
% \vspace{-\baselineskip}
\begin{enumerate}
\item кодируемое сообщение "--- $C \in A_1^+$
(на практике символы первичного алфавита $a \in A_1$ "--- байты);

\item символы 
% первичного алфавита $x \in A_1$ 
считаются независимыми:
% (сообщение рассматривается как источник данных без памяти);
$p(a) = const$ (но~$p(a_i) \neq p(a_j)$ в~общем случае для $a_i, a_j \in A_1$);

\item их вероятности оцениваются по частотам в~сообщении~$C$;
% $p(x) = \frac{\nu_x}{\sum_{y \in C} \nu_y}$;

% \item количество информации $I(a_i)$ (как и~суммарное $I(C)$, и~среднее источника $I(X)$) оценивается исходя из~оценок $p(a_i)$.

\end{enumerate}
\vspace{-0.5\baselineskip}

\hrulefill

Если $\forall a_i, a_j \in A_1$ верно $p(a_i) = p(a_j)$ "--- модель без памяти $X$ не~избыточна, энтропийное сжатие не уменьшит объёма;
% (модель другого типа может оказаться избыточной);
если~вероятности символов (байтов) не равны друг другу (и~$\frac{1}{256}$)
"--- энтропийное сжатие уменьшит объём данных приблизительно до \rlap{$I(X)$.}
% (плюс заголовок для корректного разжатия).

\vspace{-0.5\baselineskip}
\end{frame}

\subsection{Алфавитное префиксное кодирование}
\begin{frame}{\insertsubsection}
\footnotesize
\setlength{\parskip}{0.3\parskip}
\setbeamertemplate{itemize/enumerate body begin}{\footnotesize}
\setbeamertemplate{itemize/enumerate subbody begin}{\footnotesize}
\setlength{\leftmargini}{0ex} 
\begin{enumerate}
% \item Символы первичного алфавита $x \in A_1$ считаются независимыми (сообщение рассматривается как источник данных без памяти);
\item Каждому символу $a \in A_1$
сопоставляется код $code(a) \in A_2^+$,
для~двоичного кодирования "--- $A_2 = \{0, 1\}$ и~$code(a)$ "--- префиксный код из $0$ и~$1$.

\item Длина кода $code(a)$ должна быть как можно ближе к~$I(a)$ (для~двоичного кодирования "--- в~битах).
\end{enumerate}

Префиксный код = дерево
\hfill
\includegraphics[width=0.5\linewidth,valign=c]{02_sf_t}

Оптимальный код "--- сбалансированное с~учётом весов дерево.
% $|code(a)| \approx I(a)$
\end{frame}

% \section{Эффективное кодирование}
% http://compress.ru/article.aspx?id=23664#06
% Пахомов\,С.


\subsection{«Авиакатастрофа» --- длина, количество информации}
\begin{frame}{\insertsubsection}
\setlength{\leftmargini}{0ex} 

Кодируем строку $x = $«авиакатастрофа»:

\begin{itemize}

\item первичный алфавит "--- символ=тетрада (4-битный байт доски), длина "--- $n = 14$ тетрад (56 бит);

пусть есть общепринятая «естественная» кодировка

{
% \hspace*{-2em}
\small
\textcolor{gray}{Alternative vexillum codicis inf. interpretatio (AVCII):}
\begin{tabular}{@{}l@{}}
\texttt{\textvisiblespace,.!?абвгикорстф}\\
\texttt{0123456789ABCDEF}
\end{tabular}

}

\item используется 9 различных символов, \mbox{частоты {а(5), т(2), в(1), и(1), к(1), с(1), р(1), о(1), ф(1)};}

\item общее (не среднее на символ!) количество информации в~тексте (согласно модели без памяти):

$I(x) = - 5\cdot \log_2\frac{5}{14} - 2\cdot \log_2\frac{2}{14} - 7\cdot \log_2\frac{1}{14} \approx 39,7$ бит


\end{itemize}


\end{frame}


\section{Код Шеннона--Фано}


\subsection{Построение дерева Шеннона--Фано}
\begin{frame}{\insertsubsection}
Дерево Шеннона"--~Фано строится \termin{сверху вниз} (от~корневого узла к~листовым):

\begin{enumerate}
\item 
% На первом шаге 
все символы % исходной информационной последовательности 
сортируются %по убыванию или возрастанию вероятностей их появления (частоты их появления), 
по частоте; %после чего 
\item
упорядоченный ряд символов %в некотором месте 
делится на две части так, чтобы в каждой из них сумма частот символов была примерно одинакова;
\item новое деление.
\end{enumerate}

Исторически первый близкий к~оптимальному префиксный код.

Не лучше кода Хаффмана по степени сжатия и~примерно аналогичен по скорости кодирования/декодирования.

\end{frame}

\subsection{«Авиакатастрофа» --- кодирование   Шеннона--Фано}
\begin{frame}{\insertsubsection}
% «авиакатастрофа» $\to$  {а(5), т(2), в(1), и(1), к(1), с(1), р(1), о(1), ф(1)}
% \includegraphics[width=1\linewidth,valign=t]{01_sf}

\hspace*{-2em}\includegraphics[width=0.9\linewidth,valign=t]{02_sf}
\hfill\parbox[t]{0.15\linewidth} {a {00}

т­ {01}

в­ {100}

и­ {1010}

к­ {1011}

с­ {1100}

р­ {1101}

о­ {1110}

ф­ {1111}
}

$code(x)=
00100101000101100010011000111011110111100
$

$|code(x)| = 5\cdot 2 + 2\cdot 2 + 3 + 6\cdot 4 = 41$ бит

\end{frame}


\section{Код Хаффмана}

\subsection{Построение дерева Хаффмана}
\begin{frame}{\insertsubsection}
Дерево Хаффмана строится \termin{снизу вверх} (от~листовых узлов к~корневому узлу):

% Алгоритм Хаффмана подразумевает построение кодового дерева в обратном порядке, то есть снизу вверх (от листовых узлов к корневому узлу).

\begin{enumerate}
\item 
% На первом шаге 
все символы % исходной информационной последовательности 
сортируются %по убыванию или возрастанию вероятностей их появления (частоты их появления), 
по частоте (по убыванию); %после чего 

\item два 
% самых редких элемента
последних (самых редких) элемента отсортированного списка узлов 
заменяются на~новый элемент % S1
% , которому приписывается повторяемость, равная сумме повторяемостей исходных элементов. 
с~частотой, равной сумме исходных;

\item 
% Затем производится 
новая сортировка. %элементов последовательности в соответствии с их повторяемостью.
\end{enumerate}

Код Хаффмана имеет минимальную длину среди префиксных.

Не увеличивает размера исходных данных в~худшем случае.
\end{frame}

\subsection{«Авиакатастрофа» --- кодирование Хаффмана}
\begin{frame}{\insertsubsection}

\includegraphics[width=0.35\linewidth,valign=t]{02_h} 
\hspace{-2em}
\includegraphics[width=0.5\linewidth,valign=t]{02_h_t} 
\hfill\parbox[t]{0.15\linewidth} {a {0}

т­ {111}

в­ {1101}

и­ {11000}

к­ {11001}

с­ {1010}

р­ {1011}

о­ {1000}

ф­ {1001}
}

\footnotesize
\setlength{\parskip}{0.\parskip}
\setbeamertemplate{itemize/enumerate body begin}{\footnotesize}
\setbeamertemplate{itemize/enumerate subbody begin}{\footnotesize}
\setlength{\leftmargini}{0ex} 
\medskip

$code(x)=
01101110000110010111010101111011100010010
$

$|code(x)| = 5\cdot 1 + 2\cdot 3 + 5\cdot 4 + 2\cdot 5 = 41$ бит

\end{frame}


\subsection{Фактическая длина данных при префиксном кодировании}
\begin{frame}{\insertsubsection}
\setlength{\leftmargini}{0ex} 
\small
\setlength{\parskip}{0.\parskip}

\begin{itemize}
\item $code(x)=
01101110000110010111010101111011100010010
$,
$41$ бит

но записать в~файл можно только целое число байтов:
%\mbox{(здесь "--- тетрад):}

% \hspace*{-3em}
$%data(x)=
01101110000110010111010101111011100010010\textcolor{red}{000}
$,
\rlap{$44$ бита $= 11$ тетрад}


\item 
для декодирования нужно дерево/таблица кодов "---
если алгоритм построения детерминирован, то %достаточно исходных частот:
\termin{массив частот $\nu_i$:}
для байта-тетрады "--- %массив 
из 16 беззнаковых целых, для~байта-октета "--- 256;


в~естественном порядке
\begin{tabular}{@{}l@{}}
\texttt{\color{gray}\textvisiblespace
         ,.!?абвгикорстф}\\
\texttt{0000050101111121}\\
\color{gray}
\texttt{0123456789ABCDEF}
\end{tabular}.

% (32-битных? 64?)
% Размер каждой частоты "--- как размер поля длины исходного файла $n$ (32 битa? 64?),
% либо частоты нормируются: $\nu_i  \to \mathrm{round}\left( \frac{\nu_i}{\max_i(\nu_i)} \right).$

Размер $\nu_i$ "--- как размер поля $n$,
либо перенормировка $\nu_i$.


% \termin{Адаптивный (поточный) кодек} без массива частот: вначале считаем все символы равновероятными, и~код тождественен AVCII, после каждого записанного/прочитанного символа перестраиваем дерево.

\termin{Адаптивный (поточный) кодек:}  вначале считаем символы равновероятными (код=AVCII), после каждого записанного/прочитанного символа перестраиваем дерево.

% \end{itemize}
% 
% \end{frame}
% \subsection{Фактическая длина данных (2)}
% \begin{frame}{\insertsubsection}
% \setlength{\leftmargini}{0ex} 
% \small
% 
% Даже если для записи частоты используется один байт (норм.):
% 
% 16 тетрад массива частот (заголовок кодека) $+$ 11 тетрад данных $=$ 27 тетрад без основного заголовка архива.
% 
% Исходная длина $n=14$, то есть короткие файлы не имеет смысла сжимать.
% 
% \begin{itemize}

\item 
Для определения того, каким конкретно декодером пользоваться "--- соответствующее поле заголовка (№ алгоритма сжатия без контекста).
\end{itemize}
\end{frame}


\subsection{Нулевые частоты и~нормировка частот}
\begin{frame}{\insertsubsection}

% При перенормировке частот $[0, \nu_{\max}] \to [0, Max]$ возможно 

\begin{enumerate}
\item 
Нулевые значения частот могут быть отброшены при первой сортировке (как в~примерах на слайдах), и~символы с~нулевыми частотами не получат кода.

Тогда при перенормировке  частот $[0, \nu_{\max}] \to [0, Max]$ необходимо, чтобы ненулевые малые частоты не перешли в~нулевые:
$
\nu_i \to \left\{
\begin{array}{ll}
0, & \nu_i = 0,\\
\mathrm{round}\left( \frac{\nu_i - 1}{\nu_{\max} - 1} \cdot (Max - 1) \right) + 1, & \nu_i \neq 0.
\end{array}
\right.
$

\item 
Нулевые значения частот могут обрабатываться по общему алгоритму: символы с~нулевыми частотами получат коды, а~код символа с~наименьшей ненулевой частотой (последнего в~сортировке; здесь «ф») удлинится на бит.

Тогда при перенормировке ненулевая частота может стать нулевой:
$\nu_i  \to \mathrm{round}\left( \frac{\nu_i}{\nu_{\max}} \cdot {Max} \right).$
\end{enumerate}
\end{frame}



% \begin{frame}{Оптимальность алгоритма Хаффмана}
% % Система кодов, полученная с помощью алгоритма Хаффмана "--- лучшая среди всех возможных систем префиксных кодов в том плане, что длина результирующей закодированной информационной последовательности получается минимальной. 
% % То есть алгоритм Хаффмана является оптимальным.
% Код Хаффмана имеет минимальную длину среди префиксных.
% 
% Не увеличивает размера исходных данных в~худшем случае.
% 
% % Основной недостаток алгоритма Хаффмана заключается в сложности процесса построения системы кодов. 
% % % Тем не менее именно оптимальный алгоритм Хаффмана является самым распространенным алгоритмом генерации кода переменной длины и находит свое воплощение в большинстве утилит сжатия и архивации информации.
% \end{frame}

% %  Коэффициенты компрессии: 8, 1,5, 1 (Лучший,
% % средний, худший коэффициенты).
% % ?
% %  Использование: Практически не применяется в чистом
% % виде. Обычно используется как один из этапов
% % компрессии в более сложных схемах.
% % ?
% %  Симметричность: 2 (за счет того, что требует двух
% % проходов по массиву сжимаемых данных).
% % ?
% %  Характерные особенности: Единственный алгоритм,
% % который не увеличивает размера исходных данных в
% % худшем случае (если не считать необходимости
% % хранить таблицу перекодировки вместе с файлом).

\section{Арифметический (интервальный) код}
\subsection{Арифметический (интервальный) код}
\begin{frame}{\insertsubsection}
% \small

Неалфавитное неразделимое кодирование

$ C = c_0 c_1 c_2 ... c_n \to z  \in [0, 1); \hfill (0, 1)\isomorphism \Realset$

$I(z) \approx I(C)$, \hfill и~чаще всего  {$I(z) >> 64~\text{бит} > I(\mathlst{double})$}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}
% \frametitle{Литература}
% 
% \begin{itemize}
% \item Вентцель Е.\,С. Теория вероятностей: Учеб. для вузов. — 6-е изд. стер. — М.: Высш. шк., 1999.— 576 c. 
% 
% \item Пахомов\,С. Сравнение 64-битных архиваторов WinRAR 4.2, WinZip 17.0 и 7-Zip 9.30
% 
% \end{itemize}
% \end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

